{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e75ef72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.client import Config\n",
    "from io import StringIO \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import psycopg2.extras as extras\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb48a269",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VARIABLES\n",
    "\n",
    "#Postgres credentials\n",
    "pw = '[password]'\n",
    "db = \"[database]\" \n",
    "user='[user]'\n",
    "host='[host]'\n",
    "port = '5432'\n",
    "\n",
    "\n",
    "#s3 credentials\n",
    "s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
    "\n",
    "bucket_name =  \"[bucket]\"\n",
    "Prefix='orders_data/'\n",
    "suffix = 'analytics_export/'\n",
    "res = s3.list_objects_v2(\n",
    "    Bucket=bucket_name,\n",
    "    Prefix=Prefix)\n",
    "    \n",
    "\n",
    "#Storage Locations - LOCAL\n",
    "#source_path = \"../data/Webpages/\"\n",
    "#save_path = \"output/\"\n",
    "   \n",
    "\n",
    "#Storage Locations - CLOUD\n",
    "source_path = 's3://'+bucket_name+'/'+Prefix\n",
    "save_path = 'analytics_export/byroneji4734/'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "499f8789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_server_connection(host_name, user_name, user_password):\n",
    "    connection = None\n",
    "    try:\n",
    "\n",
    "        connection = psycopg2.connect(\n",
    "            database=db,\n",
    "            user=user,\n",
    "            password=pw,\n",
    "            host=host,\n",
    "            port=port\n",
    "        )\n",
    "        \n",
    "        print(\"PostgreSql Database connection successful\")\n",
    "    except Error as err:\n",
    "        print(f\"Error: '{err}'\")\n",
    "\n",
    "    return connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3586ed3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#List and Download files from data lake\n",
    "\n",
    "#def download_csv_from_s3_bucket():\n",
    "    file_list = []\n",
    "    for content in res.get('Contents', [])[1:]:\n",
    "        trunc_key = (content['Key'].rsplit(Prefix))\n",
    "        trunc_key = (','.join(trunc_key[1:]))\n",
    "        #print(trunc_key)\n",
    "        s3.download_file(bucket_name,content['Key'], trunc_key)\n",
    "        file_list.append(trunc_key[:-4]) #create list of files ignoring the file extension(.csv)\n",
    "\n",
    "    for file in file_list:\n",
    "        s3.download_file(bucket_name, \"orders_data/\"+file+\".csv\", file+\".csv\")\n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cbd0ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_convert_csv_to_dataframe(df_name,csv_path):\n",
    "\n",
    "    \n",
    "    if df_name == 'shipment_deliveries':\n",
    "        obj = s3.get_object(Bucket= bucket_name, Key= Prefix+csv_path)\n",
    "        df_name = pd.read_csv(obj['Body']) \n",
    "        df_name = pd.DataFrame(df_name)\n",
    "        #replace NaN values with a weird date string to enable SQL accept the entry\n",
    "        df_name = df_name.replace(np.nan, '1000-01-01',regex = True) \n",
    "   \n",
    "    else:\n",
    "        obj = s3.get_object(Bucket= bucket_name, Key= Prefix+csv_path)\n",
    "        df_name = pd.read_csv(obj['Body']) \n",
    "        df_name = pd.DataFrame(df_name)\n",
    "        \n",
    "    return df_name  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6289eb98",
   "metadata": {},
   "source": [
    "### LOAD DATA TO STAGING SCHEMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "feedeac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df_to_postgres(df, table):\n",
    "\n",
    "    tuples = [list(row) for row in df.itertuples(index=False)]\n",
    " \n",
    "\n",
    "    cols = ','.join(list(df.columns))\n",
    "    \n",
    "    # SQL query to execute\n",
    "    query = \"INSERT INTO %s(%s) VALUES %%s\" % (table, cols)                   \n",
    "    cursor = connection.cursor()\n",
    "    \n",
    "    try:\n",
    "        extras.execute_values(cursor, query, tuples)\n",
    "        #cursor.execute(query)\n",
    "        connection.commit()\n",
    "      \n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(\"Error: %s\" % error)\n",
    "        connection.rollback()\n",
    "        cursor.close()\n",
    "        return 1\n",
    "    except psycopg2.InterfaceError:\n",
    "        pass\n",
    "    print(\"load to postgresql successful\")\n",
    "    cursor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa471f7",
   "metadata": {},
   "source": [
    "# DATA TRANSFORMATION AND ANALYTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43ef1376",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_postgres_for_weird_dates(query):\n",
    "    cursor = connection.cursor()\n",
    "    try:\n",
    "        cursor.execute(query)\n",
    "        connection.commit()\n",
    "        print('weird dates casted to Null')\n",
    "    except Error as err:\n",
    "        print(f\"Error: '{err}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35a841ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_convert_query_to_dataframe(query):\n",
    "    cursor = connection.cursor()\n",
    "    result = None\n",
    "    try:\n",
    "        cursor.execute(query)\n",
    "        result = cursor.fetchall()\n",
    "        #get column names\n",
    "        col_names = [i[0] for i in cursor.description ]\n",
    "        result = pd.DataFrame(result,columns=col_names)\n",
    "        return result\n",
    "    except Error as err:\n",
    "        print(f\"Error: '{err}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d12e952",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_update_for_null_shipment_date = \"\"\"\n",
    "UPDATE byroneji4734_staging.shipment_deliveries\n",
    "    SET shipment_date = Null\n",
    "WHERE shipment_date = '1000-01-01';\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6bf7500",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_update_for_null_delivery_date = \"\"\"\n",
    "UPDATE byroneji4734_staging.shipment_deliveries\n",
    "    SET delivery_date = Null\n",
    "WHERE delivery_date = '1000-01-01';\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a16bbe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_agg_shipments = \"\"\"\n",
    "SELECT CURRENT_DATE AS ingestion_date, tt_late_shipments,  tt_undelivered_items\n",
    "FROM \n",
    "(\n",
    "    SELECT count(orders.order_date) AS tt_late_shipments\n",
    "    FROM byroneji4734_staging.shipment_deliveries\n",
    "\n",
    "    JOIN byroneji4734_staging.orders\n",
    "    ON orders.order_id = shipment_deliveries.order_id\n",
    "    WHERE shipment_date >= (orders.order_date + 6)\n",
    "    AND shipment_deliveries.delivery_date IS Null\n",
    ") \n",
    "    as x, \n",
    "\n",
    "(\n",
    "\n",
    "    SELECT count(orders.order_date) as tt_undelivered_items\n",
    "    FROM byroneji4734_staging.shipment_deliveries\n",
    "\n",
    "    JOIN byroneji4734_staging.orders\n",
    "    ON orders.order_id = shipment_deliveries.order_id\n",
    "    WHERE shipment_date IS Null\n",
    "    AND shipment_deliveries.delivery_date IS Null\n",
    "    AND (order_date) + 15 = '2022-09-05'\n",
    "\n",
    ") \n",
    "\n",
    "as y\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "476dca2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_agg_public_holiday = \"\"\"\n",
    "SELECT\n",
    "    CURRENT_DATE AS ingestion_date, \n",
    "    count(order_id) FILTER (WHERE month_of_the_year_num = 1) AS \"tt_order_hol_jan\",\n",
    "    count(order_id) FILTER (WHERE month_of_the_year_num = 2) AS \"tt_order_hol_feb\",\n",
    "    count(order_id) FILTER (WHERE month_of_the_year_num = 3) AS \"tt_order_hol_mar\",\n",
    "    count(order_id) FILTER (WHERE month_of_the_year_num = 4) AS \"tt_order_hol_apr\",\n",
    "    count(order_id) FILTER (WHERE month_of_the_year_num = 5) AS \"tt_order_hol_may\",\n",
    "    count(order_id) FILTER (WHERE month_of_the_year_num = 6) AS \"tt_order_hol_jun\",\n",
    "    count(order_id) FILTER (WHERE month_of_the_year_num = 7) AS \"tt_order_hol_jul\",\n",
    "    count(order_id) FILTER (WHERE month_of_the_year_num = 8) AS \"tt_order_hol_aug\",\n",
    "    count(order_id) FILTER (WHERE month_of_the_year_num = 9) AS \"tt_order_hol_sep\",\n",
    "    count(order_id) FILTER (WHERE month_of_the_year_num = 10) AS \"tt_order_hol_oct\",\n",
    "    count(order_id) FILTER (WHERE month_of_the_year_num = 11) AS \"tt_order_hol_nov\",\n",
    "    count(order_id) FILTER (WHERE month_of_the_year_num = 12) AS \"tt_order_hol_dec\"\n",
    "FROM if_common.dim_dates\n",
    "JOIN byroneji4734_staging.orders\n",
    "ON order_date = dim_dates.calendar_dt\n",
    "WHERE orders.order_date IN(\n",
    "\n",
    "    SELECT dim_dates.calendar_dt\n",
    "    FROM if_common.dim_dates\n",
    "    WHERE dim_dates.calendar_dt >= CURRENT_DATE - INTERVAL '1 year'\n",
    "    AND dim_dates.calendar_dt <= CURRENT_DATE\n",
    "    AND working_day = False\n",
    "    AND day_of_the_week_num BETWEEN 1 AND 5\n",
    "    GROUP BY month_of_the_year_num, dim_dates.calendar_dt\n",
    "    ORDER BY month_of_the_year_num\n",
    ")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7af41db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_best_performing_product = \"\"\"\n",
    "SELECT *\n",
    "\n",
    "     \n",
    "\n",
    "FROM\n",
    "\n",
    "(\n",
    "\n",
    "SELECT CURRENT_DATE AS ingestion_date, product_name, calendar_dt AS most_ordered_day,\n",
    "     bool_and(working_day = False AND day_of_the_week_num BETWEEN 1 AND 5)AS is_public_holiday,\n",
    "     sum(review) AS tt_review_points\n",
    "\n",
    "\n",
    "FROM if_common.dim_dates\n",
    "\n",
    "FULL JOIN byroneji4734_staging.shipment_deliveries\n",
    "ON shipment_deliveries.delivery_date = calendar_dt\n",
    "\n",
    "\n",
    "JOIN byroneji4734_staging.reviews\n",
    "ON reviews.product_id = \n",
    "(\n",
    "    SELECT reviews.product_id\n",
    "    FROM byroneji4734_staging.reviews\n",
    "    GROUP BY reviews.product_id\n",
    "    ORDER BY sum(review) DESC\n",
    "    LIMIT 1\n",
    "\n",
    "\n",
    ")\n",
    "\n",
    "LEFT JOIN if_common.dim_products\n",
    "ON dim_products.product_id = reviews.product_id\n",
    "\n",
    "WHERE dim_dates.calendar_dt = \n",
    "(\n",
    "\n",
    "SELECT order_date\n",
    "FROM byroneji4734_staging.orders\n",
    "\n",
    "\n",
    "WHERE orders.product_id = \n",
    "(\n",
    "    SELECT reviews.product_id\n",
    "    FROM byroneji4734_staging.reviews\n",
    "    GROUP BY reviews.product_id\n",
    "    ORDER BY sum(review) DESC\n",
    "    LIMIT 1\n",
    "    \n",
    ")\n",
    "\n",
    "GROUP BY  order_date, orders.product_id\n",
    "ORDER BY sum (quantity) DESC\n",
    "LIMIT 1\n",
    "\n",
    ")\n",
    "GROUP BY calendar_dt, product_name, reviews.product_id\n",
    "\n",
    ") AS a,\n",
    "\n",
    "\n",
    "\n",
    "(\n",
    "\n",
    "SELECT \n",
    "                  ROUND(COUNT(*) * 1.0 / SUM(COUNT(*)) \n",
    "          OVER (PARTITION BY review), 3) as pct_one_star_review\n",
    "\n",
    "FROM byroneji4734_staging.reviews\n",
    "WHERE review = 1\n",
    "GROUP BY  product_id, review\n",
    "LIMIT 1\n",
    "     \n",
    ") AS b,\n",
    "\n",
    "\n",
    "(\n",
    "\n",
    "SELECT \n",
    "                  ROUND(COUNT(*) * 1.0 / SUM(COUNT(*)) \n",
    "          OVER (PARTITION BY review=1), 3) as pct_two_star_review\n",
    "\n",
    "FROM byroneji4734_staging.reviews\n",
    "WHERE review = 2\n",
    "GROUP BY  product_id, review\n",
    "LIMIT 1\n",
    "     \n",
    ") AS c,\n",
    "\n",
    "\n",
    "(\n",
    "\n",
    "SELECT \n",
    "                  ROUND(COUNT(*) * 1.0 / SUM(COUNT(*)) \n",
    "          OVER (PARTITION BY review), 3) as pct_three_star_review\n",
    "\n",
    "FROM byroneji4734_staging.reviews\n",
    "WHERE review = 3\n",
    "GROUP BY  product_id, review\n",
    "LIMIT 1\n",
    "     \n",
    ") AS d,\n",
    "\n",
    "\n",
    "(\n",
    "\n",
    "SELECT \n",
    "                  ROUND(COUNT(*) * 1.0 / SUM(COUNT(*)) \n",
    "          OVER (PARTITION BY review), 3) as pct_four_star_review\n",
    "\n",
    "FROM byroneji4734_staging.reviews\n",
    "WHERE review = 4\n",
    "GROUP BY  product_id, review\n",
    "LIMIT 1\n",
    "     \n",
    ") AS e,\n",
    "\n",
    "\n",
    "(\n",
    "\n",
    "SELECT \n",
    "                  ROUND(COUNT(*) * 1.0 / SUM(COUNT(*)) \n",
    "          OVER (PARTITION BY review), 3) as pct_five_star_review\n",
    "\n",
    "FROM byroneji4734_staging.reviews\n",
    "WHERE review = 5\n",
    "GROUP BY  product_id, review\n",
    "LIMIT 1\n",
    "     \n",
    ") AS f,\n",
    "\n",
    "\n",
    "(\n",
    "\n",
    "\n",
    "     SELECT    1 - (count(orders.order_id) FILTER (WHERE shipment_date >= (orders.order_date + 6)\n",
    "                AND shipment_deliveries.delivery_date IS Null AND orders.product_id = (\n",
    "\n",
    "                            SELECT reviews.product_id\n",
    "                            FROM byroneji4734_staging.reviews\n",
    "                            GROUP BY reviews.product_id\n",
    "                            ORDER BY sum(review) DESC\n",
    "                            LIMIT 1\n",
    "\n",
    "                )\n",
    "                \n",
    "                )/CAST(COUNT(orders.order_id) AS float)) AS pct_early_shipments,\n",
    "\n",
    "\n",
    "                ((count(orders.order_id) FILTER (WHERE shipment_date >= (orders.order_date + 6)\n",
    "                AND shipment_deliveries.delivery_date IS Null AND orders.product_id = (\n",
    "\n",
    "                            SELECT reviews.product_id\n",
    "                            FROM byroneji4734_staging.reviews\n",
    "                            GROUP BY reviews.product_id\n",
    "                            ORDER BY sum(review) DESC\n",
    "                            LIMIT 1\n",
    "\n",
    "                )\n",
    "                \n",
    "                )/CAST(COUNT(orders.order_id) AS float))) AS pct_late_shipments        \n",
    "    \n",
    "\n",
    "\n",
    "    FROM byroneji4734_staging.shipment_deliveries\n",
    "\n",
    "    JOIN byroneji4734_staging.orders\n",
    "    ON orders.order_id = shipment_deliveries.order_id\n",
    "    WHERE shipment_date >= (orders.order_date + 6)\n",
    "    AND shipment_deliveries.delivery_date IS Null\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    ") as g\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86102f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_all_agg_public_holiday = \"\"\"\n",
    "SELECT *\n",
    "FROM byroneji4734_analytics.agg_public_holiday\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84923698",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_all_agg_shipments = \"\"\"\n",
    "SELECT *\n",
    "FROM byroneji4734_analytics.agg_shipments\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbd90927",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_all_best_performing_product = \"\"\"\n",
    "SELECT *\n",
    "FROM byroneji4734_analytics.best_performing_product\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc3ffda",
   "metadata": {},
   "source": [
    "LOADING ANALYSIS DATAFRAME TO S3 BUCKET AS CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f9417b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_dataframe_bucket_as_csv(df_file,file_name):\n",
    "    \"\"\"Upload a file to an S3 bucket\n",
    "\n",
    "    :param df_file: Dataframe to process\n",
    "    :param file_name: Desired name of file in the bucket\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Upload the file\n",
    "    \n",
    "    csv_buffer = StringIO()\n",
    "    df_file.to_csv(csv_buffer)\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    \n",
    "    try:\n",
    "        s3_resource.Object(bucket_name, save_path+file_name+'.csv').put(Body=csv_buffer.getvalue())\n",
    "        print('Successfully uploaded dataframe as CSV to S3 bucket')\n",
    "    except :\n",
    "        #logging.error(e)\n",
    "        print('failed to upload csv to s3')\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d25b4395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PostgreSql Database connection successful\n",
      "Error: duplicate key value violates unique constraint \"orders_pkey\"\n",
      "DETAIL:  Key (order_id)=(1) already exists.\n",
      "\n",
      "load to postgresql successful\n",
      "Error: duplicate key value violates unique constraint \"shipment_deliveries_pkey\"\n",
      "DETAIL:  Key (shipment_id)=(1) already exists.\n",
      "\n",
      "weird dates casted to Null\n",
      "weird dates casted to Null\n",
      "Error: duplicate key value violates unique constraint \"agg_shipments_pkey\"\n",
      "DETAIL:  Key (ingestion_date)=(2022-10-02) already exists.\n",
      "\n",
      "Error: duplicate key value violates unique constraint \"agg_public_holiday_pkey\"\n",
      "DETAIL:  Key (ingestion_date)=(2022-10-02) already exists.\n",
      "\n",
      "Error: duplicate key value violates unique constraint \"best_performing_product_pkey\"\n",
      "DETAIL:  Key (ingestion_date)=(2022-10-02) already exists.\n",
      "\n",
      "Successfully uploaded dataframe as CSV to S3 bucket\n",
      "Successfully uploaded dataframe as CSV to S3 bucket\n",
      "Successfully uploaded dataframe as CSV to S3 bucket\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    connection = create_server_connection(host, user, pw)\n",
    "\n",
    "    #download_csv_from_s3_bucket()\n",
    "\n",
    "    orders = read_and_convert_csv_to_dataframe('orders','orders.csv')\n",
    "\n",
    "    reviews = read_and_convert_csv_to_dataframe('reviews','reviews.csv')\n",
    "\n",
    "    shipment_deliveries = read_and_convert_csv_to_dataframe('shipment_deliveries','shipment_deliveries.csv')\n",
    "\n",
    "    load_df_to_postgres(orders, 'byroneji4734_staging.orders')\n",
    "\n",
    "    load_df_to_postgres(reviews, 'byroneji4734_staging.reviews')\n",
    "\n",
    "    load_df_to_postgres(shipment_deliveries, 'byroneji4734_staging.shipment_deliveries')\n",
    "\n",
    "    update_postgres_for_weird_dates(query_update_for_null_shipment_date)\n",
    "\n",
    "    update_postgres_for_weird_dates(query_update_for_null_delivery_date)\n",
    "\n",
    "    agg_shipments = read_and_convert_query_to_dataframe(query_agg_shipments)\n",
    "\n",
    "    agg_public_holiday = read_and_convert_query_to_dataframe(query_agg_public_holiday)\n",
    "\n",
    "    best_performing_product = read_and_convert_query_to_dataframe(query_best_performing_product)\n",
    "\n",
    "\n",
    "    load_df_to_postgres(agg_shipments, 'byroneji4734_analytics.agg_shipments')\n",
    "\n",
    "    load_df_to_postgres(agg_public_holiday, 'byroneji4734_analytics.agg_public_holiday')\n",
    "\n",
    "    load_df_to_postgres(best_performing_product, 'byroneji4734_analytics.best_performing_product')\n",
    "\n",
    "\n",
    "\n",
    "    all_agg_shipments = read_and_convert_query_to_dataframe(query_all_agg_shipments)\n",
    "\n",
    "    all_agg_public_holiday = read_and_convert_query_to_dataframe(query_all_agg_public_holiday)\n",
    "\n",
    "    all_best_performing_product = read_and_convert_query_to_dataframe(query_all_best_performing_product)\n",
    "\n",
    "\n",
    "    upload_to_dataframe_bucket_as_csv(all_agg_shipments,'agg_shipments')\n",
    "\n",
    "    upload_to_dataframe_bucket_as_csv(all_agg_public_holiday,'agg_public_holiday')\n",
    "\n",
    "    upload_to_dataframe_bucket_as_csv(all_best_performing_product,'best_performing_product')\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
